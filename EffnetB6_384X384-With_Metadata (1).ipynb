{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch\n",
    "# !pip install pretrainedmodels\n",
    "# !pip install albumentations\n",
    "# !pip install pandas\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import albumentations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import pretrainedmodels\n",
    "\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>stratify_group</th>\n",
       "      <th>fold</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC_2637011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id   sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968  male        45.0                     head/neck   \n",
       "\n",
       "  diagnosis benign_malignant  target  source  stratify_group  fold  \\\n",
       "0   unknown           benign       0  ISIC20              20     0   \n",
       "\n",
       "       image_id  \n",
       "0  ISIC_2637011  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.read_csv('../input/FinalFolds_combinedExternal.csv')\n",
    "df_folds['image_id']=df_folds['image_name']\n",
    "df_folds.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0052060</td>\n",
       "      <td>IP_3579794</td>\n",
       "      <td>male</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id   sex  age_approx anatom_site_general_challenge\n",
       "0  ISIC_0052060  IP_3579794  male        70.0                           NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/384X384/test.csv')\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['male', 'female', 'unknown_sex'], dtype=object),\n",
       " array(['male', 'female'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds['sex'] = df_folds['sex'].replace(\"unknown\",'unknown_sex')\n",
    "test_df['sex'] = test_df['sex'].replace(\"unknown\",'unknown_sex')\n",
    "df_folds['sex'].unique(),test_df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill missing values\n",
    "df_folds['sex'].fillna('unknown_sex',inplace = True)\n",
    "test_df['sex'].fillna('unknown_sex',inplace = True)\n",
    "\n",
    "df_folds['anatom_site_general_challenge'].fillna('unknown',inplace = True)\n",
    "test_df['anatom_site_general_challenge'].fillna('unknown',inplace = True)\n",
    "\n",
    "df_folds['age_approx'] = df_folds['age_approx'].fillna(df_folds['age_approx'].mode().values[0])\n",
    "test_df['age_approx']  = test_df['age_approx'].fillna(test_df['age_approx'].mode().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     image_name  patient_id   sex  age_approx anatom_site_general_challenge  \\\n",
      "0  ISIC_2637011  IP_7279968  male        45.0                     head/neck   \n",
      "\n",
      "  diagnosis benign_malignant  target  source  stratify_group  fold  \\\n",
      "0   unknown           benign       0  ISIC20              20     0   \n",
      "\n",
      "       image_id  \n",
      "0  ISIC_2637011        image_name  patient_id   sex  age_approx anatom_site_general_challenge\n",
      "0  ISIC_0052060  IP_3579794  male        70.0                       unknown\n"
     ]
    }
   ],
   "source": [
    "print(df_folds.head(1),test_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  8.18it/s]\n"
     ]
    }
   ],
   "source": [
    "### one hot enc for sex, age approx, anatom site general challenge\n",
    "\n",
    "import gc\n",
    "def getDummies(col,df_folds=df_folds,test_df=test_df):\n",
    "    _ = pd.concat([df_folds[col],test_df[col]],0)\n",
    "    _.reset_index(drop=True,inplace=True)\n",
    "    if col == 'age_approx':\n",
    "        prefix='age'\n",
    "    else:\n",
    "        prefix=None\n",
    "    dummies = pd.get_dummies(_,prefix=prefix,dtype=np.float64)\n",
    "    dummies.tail()\n",
    "    dummies.iloc[len(df_folds):,:]\n",
    "\n",
    "    tt = dummies.iloc[len(df_folds):,:]\n",
    "    tt.reset_index(drop=True,inplace=True)\n",
    "    test_df = pd.concat([test_df,tt],1)\n",
    "\n",
    "    tt = dummies.iloc[:len(df_folds),:]\n",
    "    tt.reset_index(drop=True,inplace=True)\n",
    "    df_folds = pd.concat([df_folds,tt],1)\n",
    "    del tt, dummies,_\n",
    "    gc.collect()\n",
    "    return df_folds,test_df\n",
    "\n",
    "from tqdm import tqdm\n",
    "for cc in tqdm(['sex','age_approx','anatom_site_general_challenge']):\n",
    "    df_folds,test_df = getDummies(cc,df_folds,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_folds['n_images'] = df_folds.patient_id.map(df_folds.groupby(['patient_id']).image_name.count())\n",
    "test_df['n_images'] = test_df.patient_id.map(test_df.groupby(['patient_id']).image_name.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general_challenge</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>stratify_group</th>\n",
       "      <th>fold</th>\n",
       "      <th>image_id</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>unknown_sex</th>\n",
       "      <th>age_0.0</th>\n",
       "      <th>age_5.0</th>\n",
       "      <th>age_10.0</th>\n",
       "      <th>age_15.0</th>\n",
       "      <th>age_20.0</th>\n",
       "      <th>age_25.0</th>\n",
       "      <th>age_30.0</th>\n",
       "      <th>age_35.0</th>\n",
       "      <th>age_40.0</th>\n",
       "      <th>age_45.0</th>\n",
       "      <th>age_50.0</th>\n",
       "      <th>age_51.0</th>\n",
       "      <th>age_55.0</th>\n",
       "      <th>age_60.0</th>\n",
       "      <th>age_65.0</th>\n",
       "      <th>age_70.0</th>\n",
       "      <th>age_75.0</th>\n",
       "      <th>age_80.0</th>\n",
       "      <th>age_85.0</th>\n",
       "      <th>age_90.0</th>\n",
       "      <th>anterior torso</th>\n",
       "      <th>head/neck</th>\n",
       "      <th>lateral torso</th>\n",
       "      <th>lower extremity</th>\n",
       "      <th>oral/genital</th>\n",
       "      <th>palms/soles</th>\n",
       "      <th>posterior torso</th>\n",
       "      <th>torso</th>\n",
       "      <th>unknown</th>\n",
       "      <th>upper extremity</th>\n",
       "      <th>n_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>IP_7279968</td>\n",
       "      <td>male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>IP_3075186</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>IP_2842074</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>nevus</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>IP_6890425</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>IP_8723313</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>upper extremity</td>\n",
       "      <td>unknown</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "      <td>ISIC20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
       "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
       "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
       "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
       "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
       "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
       "\n",
       "  diagnosis benign_malignant  target  source  stratify_group  fold  \\\n",
       "0   unknown           benign       0  ISIC20              20     0   \n",
       "1   unknown           benign       0  ISIC20               2     1   \n",
       "2     nevus           benign       0  ISIC20               0     4   \n",
       "3   unknown           benign       0  ISIC20               2     3   \n",
       "4   unknown           benign       0  ISIC20               1     3   \n",
       "\n",
       "       image_id  female  male  unknown_sex  age_0.0  age_5.0  age_10.0  \\\n",
       "0  ISIC_2637011     0.0   1.0          0.0      0.0      0.0       0.0   \n",
       "1  ISIC_0015719     1.0   0.0          0.0      0.0      0.0       0.0   \n",
       "2  ISIC_0052212     1.0   0.0          0.0      0.0      0.0       0.0   \n",
       "3  ISIC_0068279     1.0   0.0          0.0      0.0      0.0       0.0   \n",
       "4  ISIC_0074268     1.0   0.0          0.0      0.0      0.0       0.0   \n",
       "\n",
       "   age_15.0  age_20.0  age_25.0  age_30.0  age_35.0  age_40.0  age_45.0  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "4       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   age_50.0  age_51.0  age_55.0  age_60.0  age_65.0  age_70.0  age_75.0  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2       1.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4       0.0       0.0       1.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   age_80.0  age_85.0  age_90.0  anterior torso  head/neck  lateral torso  \\\n",
       "0       0.0       0.0       0.0             0.0        1.0            0.0   \n",
       "1       0.0       0.0       0.0             0.0        0.0            0.0   \n",
       "2       0.0       0.0       0.0             0.0        0.0            0.0   \n",
       "3       0.0       0.0       0.0             0.0        1.0            0.0   \n",
       "4       0.0       0.0       0.0             0.0        0.0            0.0   \n",
       "\n",
       "   lower extremity  oral/genital  palms/soles  posterior torso  torso  \\\n",
       "0              0.0           0.0          0.0              0.0    0.0   \n",
       "1              0.0           0.0          0.0              0.0    0.0   \n",
       "2              1.0           0.0          0.0              0.0    0.0   \n",
       "3              0.0           0.0          0.0              0.0    0.0   \n",
       "4              0.0           0.0          0.0              0.0    0.0   \n",
       "\n",
       "   unknown  upper extremity  n_images  \n",
       "0      0.0              0.0       115  \n",
       "1      0.0              1.0        24  \n",
       "2      0.0              0.0         5  \n",
       "3      0.0              0.0        22  \n",
       "4      0.0              1.0        20  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',100)\n",
    "df_folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'patient_id', 'sex', 'age_approx',\n",
       "       'anatom_site_general_challenge', 'diagnosis', 'benign_malignant',\n",
       "       'target', 'source', 'stratify_group', 'fold', 'image_id', 'female',\n",
       "       'male', 'unknown_sex', 'age_0.0', 'age_5.0', 'age_10.0', 'age_15.0',\n",
       "       'age_20.0', 'age_25.0', 'age_30.0', 'age_35.0', 'age_40.0', 'age_45.0',\n",
       "       'age_50.0', 'age_51.0', 'age_55.0', 'age_60.0', 'age_65.0', 'age_70.0',\n",
       "       'age_75.0', 'age_80.0', 'age_85.0', 'age_90.0', 'anterior torso',\n",
       "       'head/neck', 'lateral torso', 'lower extremity', 'oral/genital',\n",
       "       'palms/soles', 'posterior torso', 'torso', 'unknown', 'upper extremity',\n",
       "       'n_images'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['female','male', 'unknown', 'age_0.0', 'age_5.0', 'age_10.0', 'age_15.0',\n",
    "        'age_20.0', 'age_25.0', 'age_30.0', 'age_35.0', 'age_40.0', 'age_45.0',\n",
    "        'age_50.0', 'age_51.0', 'age_55.0', 'age_60.0', 'age_65.0', 'age_70.0',\n",
    "        'age_75.0', 'age_80.0', 'age_85.0', 'age_90.0', 'anterior torso',\n",
    "        'head/neck', 'lateral torso', 'lower extremity', 'oral/genital',\n",
    "        'palms/soles', 'posterior torso', 'torso', 'unknown', 'upper extremity','n_images']\n",
    "\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58457, 34)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds[cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class ClassificationLoader:\n",
    "    def __init__(self, image_paths, targets, resize,tabularDF=df_folds,cols=cols,augmentations=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "        self.tabularDF = tabularDF\n",
    "        self.cols = cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item])\n",
    "        targets = self.targets[item]\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "        image = np.array(image)\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        tabFeats = self.tabularDF.loc[item,cols].values.astype(float)\n",
    "#         print(tabFeats)\n",
    "        \n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "            \"tabfeats\":torch.tensor(np.array(tabFeats),dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {} out of {}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\n",
    "                \"Validation score improved ({} --> {}). Saving model!\".format(\n",
    "                    self.val_score, epoch_score\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "        \n",
    "    @staticmethod\n",
    "    def train(\n",
    "        data_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler=None,\n",
    "        accumulation_steps=1,\n",
    "        use_tpu=False,\n",
    "        fp16=False,\n",
    "    ):\n",
    "\n",
    "        try:\n",
    "            from apex import amp\n",
    "            _apex_available = True\n",
    "        except ImportError:\n",
    "            _apex_available = False\n",
    "\n",
    "        if use_tpu and not _xla_available:\n",
    "            raise Exception(\n",
    "                \"You want to use TPUs but you dont have pytorch_xla installed\"\n",
    "            )\n",
    "        if fp16 and not _apex_available:\n",
    "            raise Exception(\"You want to use fp16 but you dont have apex installed\")\n",
    "        if fp16 and use_tpu:\n",
    "            raise Exception(\"Apex fp16 is not available when using TPUs\")\n",
    "        if fp16:\n",
    "            accumulation_steps = 1\n",
    "        losses = AverageMeter()\n",
    "        predictions = []\n",
    "        model.train()\n",
    "        if accumulation_steps > 1:\n",
    "            optimizer.zero_grad()\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(device)\n",
    "            if accumulation_steps == 1 and b_idx == 0:\n",
    "                optimizer.zero_grad()\n",
    "            _, loss = model(**data)\n",
    "\n",
    "            if not use_tpu:\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    if fp16:\n",
    "                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                            scaled_loss.backward()\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "                    if (b_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "                        if b_idx > 0:\n",
    "                            optimizer.zero_grad()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                xm.optimizer_step(optimizer)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                if b_idx > 0:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            losses.update(loss.item(), data_loader.batch_size)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "        return losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(data_loader, model, device, use_tpu=False):\n",
    "        losses = AverageMeter()\n",
    "        final_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, loss = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                losses.update(loss.item(), data_loader.batch_size)\n",
    "                final_predictions.append(predictions)\n",
    "                tk0.set_postfix(loss=losses.avg)\n",
    "        return final_predictions, losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(data_loader, model, device, use_tpu=False):\n",
    "        model.eval()\n",
    "        final_predictions = []\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, _ = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                final_predictions.append(predictions)\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self, sz=None):\n",
    "        super().__init__()\n",
    "        sz = sz or (1,1)\n",
    "        self.ap = nn.AdaptiveAvgPool2d(sz)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(sz)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = 0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        \n",
    "        target = target* (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        loss = F.binary_cross_entropy_with_logits(x, target.type_as(x))\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, arch):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.arch = arch\n",
    "        \n",
    "#         in_features  = arch._fc.in_features\n",
    "#         self.arch._fc = nn.Linear(in_features=in_features, out_features=1, bias=True)\n",
    "                                \n",
    "#     def forward(self, image,targets):\n",
    "#         \"\"\"\n",
    "#         No sigmoid in forward because we are going to use BCEWithLogitsLoss\n",
    "#         Which applies sigmoid for us when calculating a loss\n",
    "#         \"\"\"\n",
    "#         batch_size, _, _, _ = image.shape\n",
    "#         x = image\n",
    "# #         criterion = nn.BCEWithLogitsLoss()\n",
    "#         criterion = LabelSmoothing()\n",
    "#         ### https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "#         output = self.arch(x)\n",
    "#         loss = criterion(output, targets.view(-1,1).float())\n",
    "#         return output,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, arch):\n",
    "        super(Net, self).__init__()\n",
    "        self.arch = arch\n",
    "        \n",
    "        in_features  = arch._fc.in_features\n",
    "        self.arch._fc = nn.Linear(in_features=in_features, out_features=256, bias=True)\n",
    "                \n",
    "        self.tabDense = nn.Linear(in_features = 34, out_features = 256,bias=True) ### 34 tabular features\n",
    "        self.imgBn = nn.LayerNorm(256)\n",
    "        self.fcout_1 = nn.Linear(512,128)\n",
    "        self.fcout_2 = nn.Linear(128,1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, image,targets,tabfeats):\n",
    "        \"\"\"\n",
    "        No sigmoid in forward because we are going to use BCEWithLogitsLoss\n",
    "        Which applies sigmoid for us when calculating a loss\n",
    "        \"\"\"\n",
    "        batch_size, _, _, _ = image.shape\n",
    "        x = image\n",
    "        criterion = LabelSmoothing()\n",
    "        ### https://github.com/clovaai/CutMix-PyTorch/blob/master/train.py\n",
    "        output_image = self.arch(x)\n",
    "        output_image = self.imgBn(output_image.unsqueeze(0).unsqueeze(0))\n",
    "        output_image = output_image.squeeze(0).squeeze(0)\n",
    "\n",
    "        output_tabular = self.tabDense(tabfeats)\n",
    "        x = torch.cat((output_image, output_tabular), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fcout_1(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fcout_2(x)\n",
    "        loss = criterion(output, targets.view(-1,1).float())\n",
    "        return output,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Microscope(A.ImageOnlyTransform):\n",
    "    def __init__(self, p: float = 0.5, always_apply=False):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        if random.random() < self.p:\n",
    "            circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8),\n",
    "                        (img.shape[0]//2, img.shape[1]//2),\n",
    "                        random.randint(img.shape[0]//2 - 3, img.shape[0]//2 + 15),\n",
    "                        (0, 0, 0),\n",
    "                        -1)\n",
    "\n",
    "            mask = circle - 255\n",
    "            img = np.multiply(img, mask)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "\n",
    "def train(fold,bs,epochs,fp16,sz,arch='se_resnet152',debug=False,accumulation_steps=1):\n",
    "    if sz is not None:\n",
    "        sz = (sz,sz)\n",
    "    else:\n",
    "        sz = None\n",
    "    \n",
    "    _n = arch\n",
    "    import os\n",
    "#     training_data_path = '../input/128X128/train/'\n",
    "    training_data_path = '../input/384X384/train/'\n",
    "    df = df_folds.copy()\n",
    "    device = \"cuda\"\n",
    "    epochs = epochs\n",
    "    train_bs = bs\n",
    "    valid_bs = bs//2\n",
    "\n",
    "    df_train = df[df.fold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.fold == fold].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    arch = EfficientNet.from_pretrained(arch)\n",
    "#     arch = pretrainedmodels.__dict__[arch](num_classes=1000, pretrained='imagenet')\n",
    "    model = Net(arch=arch)  # New model for each fold\n",
    "    model = model.to(device)     \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    train_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
    "            albumentations.CoarseDropout(),\n",
    "            albumentations.RandomBrightness(0.3),\n",
    "            albumentations.RandomContrast(0.3),\n",
    "            albumentations.ChannelShuffle(),\n",
    "            albumentations.Cutout(4,4,4),\n",
    "            Microscope(),\n",
    "            albumentations.ChannelDropout(p=0.1),\n",
    "            albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=30),\n",
    "            albumentations.Flip(p=0.5)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        train_images = df_train.image_id.values.tolist()[:250]\n",
    "        train_images = [os.path.join(training_data_path, i + \".jpg\") for i in train_images]\n",
    "        train_targets = df_train.target.values[:250]\n",
    "\n",
    "        valid_images = df_valid.image_id.values.tolist()[:250]\n",
    "        valid_images = [os.path.join(training_data_path, i + \".jpg\") for i in valid_images]\n",
    "        valid_targets = df_valid.target.values[:250]\n",
    "    else:\n",
    "        train_images = df_train.image_id.values.tolist()\n",
    "        train_images = [os.path.join(training_data_path, i + \".jpg\") for i in train_images]\n",
    "        train_targets = df_train.target.values\n",
    "\n",
    "        valid_images = df_valid.image_id.values.tolist()\n",
    "        valid_images = [os.path.join(training_data_path, i + \".jpg\") for i in valid_images]\n",
    "        valid_targets = df_valid.target.values\n",
    "        \n",
    "    train_dataset = ClassificationLoader(\n",
    "        image_paths=train_images,\n",
    "        targets=train_targets,\n",
    "        resize=sz,\n",
    "        tabularDF = df_train,\n",
    "        cols = cols,\n",
    "        augmentations=train_aug,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=train_bs, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = ClassificationLoader(\n",
    "        image_paths=valid_images,\n",
    "        targets=valid_targets,\n",
    "        resize=sz,\n",
    "        tabularDF = df_valid,\n",
    "        cols = cols,\n",
    "        augmentations=valid_aug)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=valid_bs, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=2,\n",
    "        threshold=0.001,\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    if fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    es = EarlyStopping(patience=3, mode=\"max\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = Engine.train(train_loader, model, optimizer, device=device,fp16=fp16,accumulation_steps=accumulation_steps)\n",
    "        predictions, valid_loss = Engine.evaluate(\n",
    "            valid_loader, model, device=device\n",
    "        )\n",
    "        predictions = np.vstack((predictions)).ravel()\n",
    "        auc = metrics.roc_auc_score(valid_targets, predictions)\n",
    "        print(f\"Epoch = {epoch}, AUC = {auc}\")\n",
    "        scheduler.step(auc)\n",
    "        \n",
    "        if sz is not None:\n",
    "            ss = sz[0]\n",
    "        else:\n",
    "            ss = 384\n",
    "            \n",
    "        es(auc, model, model_path= \"../models/model_tabData_arch_{}_sz_{}_fold_{}_epoch_{}_auc_{}.bin\".format(_n,ss,fold,epoch,round(auc*100,2)))\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3335 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 49/3335 [00:26<26:32,  2.06it/s, loss=0.324] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 52/3335 [00:27<25:42,  2.13it/s, loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 123/3335 [01:04<27:39,  1.94it/s, loss=0.335]"
     ]
    }
   ],
   "source": [
    "e = 25\n",
    "debug= False\n",
    "bs = 14\n",
    "accumulation_steps = 1\n",
    "mtype = 'efficientnet-b6'\n",
    "from apex import amp, optimizers\n",
    "apx = True\n",
    "\n",
    "train(0,bs,e,apx,None,mtype,debug=debug,accumulation_steps=accumulation_steps)\n",
    "train(1,bs,e,apx,None,mtype,debug=debug,accumulation_steps=accumulation_steps)\n",
    "train(2,bs,e,apx,None,mtype,debug=debug,accumulation_steps=accumulation_steps)\n",
    "train(3,bs,e,apx,None,mtype,debug=debug,accumulation_steps=accumulation_steps)\n",
    "train(4,bs,e,apx,None,mtype,debug=debug,accumulation_steps=accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
